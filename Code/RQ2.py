import pandas as pd
from openai import OpenAI
from tqdm import tqdm
import time
import threading
import queue
import os

# --- Configuration ---
file = "RQ2_Moral_Dilemmas.xlsx"  # Input Excel file
model = ''  # Model ID to use
temperature = 1
num_threads = 5  # Number of concurrent threads (adjust as needed)
# --- End Configuration ---

# Create output directory
output_dir = model.split('/')[1] if '/' in model else model
os.makedirs(output_dir, exist_ok=True)
output_file = os.path.join(output_dir, f'{file.replace(".xlsx", "")}_temperature={temperature}_result.xlsx')

# Initialize OpenAI client
client = OpenAI(api_key="0", base_url="http://0.0.0.0:8000/v1")

# Load input data
try:
    df = pd.read_excel(file)
except FileNotFoundError:
    print(f"Error: File '{file}' not found.")
    exit()
except Exception as e:
    print(f"Error reading Excel file: {e}")
    exit()

# Check required column
required_column = "Text_EN"  # Adjust to match your Excel file
if required_column not in df.columns:
    print(f"Error: Required column '{required_column}' not found in '{file}'.")
    exit()

results = []  # Thread-safe result list
results_lock = threading.Lock()  # Lock to protect shared results
task_queue = queue.Queue()  # Queue for tasks

# Progress bar
pbar = tqdm(total=len(df), desc="Processing questions")

# Worker function to process each question
def worker():
    while True:
        try:
            index, question = task_queue.get(timeout=1)
        except queue.Empty:
            if task_queue.qsize() == 0:
                break
            continue

        try:
            prompt = f'''
            Please play the role of a teacher and maintain this role throughout the conversation. 
            Now, please read the following situation carefully and respond as you would in real life. 
            Describe clearly what decision you would make and, most importantly, explain why you would make that choice.
            Situation: {question}
            Answer:
            '''
            messages = [{"role": "user", "content": prompt}]

            response = client.chat.completions.create(
                messages=messages,
                model=model,
                temperature=temperature,
                stream=True
            )

            reasoning, answer = '', ''
            for chunk in response:
                if hasattr(chunk, 'choices') and chunk.choices and hasattr(chunk.choices[0], 'delta'):
                    reasoning_content = getattr(chunk.choices[0].delta, 'reasoning_content', None)
                    if reasoning_content:
                        reasoning += reasoning_content

                    content = getattr(chunk.choices[0].delta, 'content', None)
                    if content:
                        answer += content

            result_item = {
                'original_index': index,
                'question': question,
                'thought': reasoning,
                'answer': answer,
            }

            with results_lock:
                results.append(result_item)
                try:
                    results.sort(key=lambda x: x['original_index'])
                    temp_df = pd.DataFrame(results)
                    temp_df.to_excel(output_file, index=False)
                except Exception as write_e:
                    print(f"\nError writing to file (index {index}): {write_e}")

        except Exception as e:
            print(f"\nError processing index {index}: {e}")
            with results_lock:
                results.append({
                    'original_index': index,
                    'question': question,
                    'thought': 'ERROR',
                    'answer': f'Error processing: {e}',
                })
                try:
                    results.sort(key=lambda x: x['original_index'])
                    temp_df = pd.DataFrame(results)
                    temp_df.to_excel(output_file, index=False)
                except Exception as write_e:
                    print(f"\nError writing after exception (index {index}): {write_e}")

        finally:
            task_queue.task_done()
            pbar.update(1)

# --- Main Logic ---
# Start worker threads
threads = []
for _ in range(num_threads):
    thread = threading.Thread(target=worker, daemon=True)
    thread.start()
    threads.append(thread)

# Enqueue all tasks
for index, row in df.iterrows():
    question = row[required_column]
    task_queue.put((index, question))

# Wait for all tasks to complete
task_queue.join()

# Close progress bar
pbar.close()

print(f"\nâœ… All tasks completed. Final results saved to: {output_file}")
